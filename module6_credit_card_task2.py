# -*- coding: utf-8 -*-
"""Module6:credit_card_task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M0v8QvvmqqOdy9jz_lkDLzxA9vCXPxKn
"""

#============= TASK-1 ==================#

#For data
import numpy as np
import pandas as pd

#For Charts
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt

#Preprocessing
from sklearn.preprocessing import OneHotEncoder, LabelEncoder,PolynomialFeatures
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import make_pipeline

#Regression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from xgboost import XGBRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.svm import SVR

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.decomposition import PCA
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score

data = pd.read_excel("/content/house_club.xlsx")
data.head()

data.info()

data.describe()

data.isnull().sum()

plt.figure(figsize=(8,6))
sns.histplot(data['Price'], kde=True)
plt.title('SalePrice distribution')
plt.show()

#2. Build a Simple Linear Regression model to predict the Sale price of the house. Use Area as the independent variable.
df1 = data

x = df1[['Area']]
x

y = df1[['Price']]
y

x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=42,test_size=0.2)

liner = LinearRegression()
liner_model = liner.fit(x_train,y_train)

def result(model):
  y_pre = model.predict(x_test)
  mse = mean_squared_error(y_test,y_pre)
  r2 = r2_score(y_test, y_pre)
  return mse,r2

models = [liner_model]
for i in models:
  mse, r2 = result(i)
  print("\tmodel: ",i)
  print("MSE: ",mse)
  print("R2: ",r2)

#3. Build Multiple Linear Regression model to predict Sale price of the house.
df2 = data

lcoder= LabelEncoder()
data['City_label'] = lcoder.fit_transform(data['City'])
data['Location_label'] = lcoder.fit_transform(data['Location'])

df2

x = df2.drop(['City','Location','Price'],axis=1)
y = df2['Price']

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

x_train, x_test,y_train, y_test = train_test_split(x,y,random_state=42,test_size=0.2)

liner = LinearRegression()
liner_model = liner.fit(x_train,y_train)

def result(model):
  y_pre = model.predict(x_test)
  mse = mean_squared_error(y_test,y_pre)
  r2 = r2_score(y_test, y_pre)
  return mse,r2

models = [liner_model]
for i in models:
  mse, r2 = result(i)
  print("\tmodel: ",i)
  print("MSE: ",mse)
  print("R2: ",r2)

#4. Use dimensionality reduction technique PCA/LDA and build Multiple Linear
#Regression model to predict Sale price of the house.
df3 =data

lcoder= LabelEncoder()
data['City_label'] = lcoder.fit_transform(data['City'])
data['Location_label'] = lcoder.fit_transform(data['Location'])

x = df2.drop(['City','Location','Price'],axis=1)


y = df2['Price']

x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=42,test_size=0.2)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

pca = PCA(n_components=2)
x_train_pca = pca.fit_transform(x_train_scaled)
x_test_pca = pca.transform(x_test_scaled)

print("PCA Explained Variance Ratio:", pca.explained_variance_ratio_)

liner = LinearRegression()
liner_model = liner.fit(x_train,y_train)

def result(model):
  y_pre = model.predict(x_test)
  mse = mean_squared_error(y_test,y_pre)
  r2 = r2_score(y_test, y_pre)
  return mse,r2

models = [liner_model]
for i in models:
  mse, r2 = result(i)
  print("\tmodel: ",i)
  print("MSE: ",mse)
  print("R2: ",r2)

#5. Build a model using Lasso and Ridge regression to reduce model complexity.
df4 = data

lcoder= LabelEncoder()
data['City_label'] = lcoder.fit_transform(data['City'])
data['Location_label'] = lcoder.fit_transform(data['Location'])

x = df2.drop(['City','Location','Price'],axis=1)
y = df2['Price']

scaler = StandardScaler()
Xtrain_scaled = scaler.fit_transform(x_train)
Xtest_scaled = scaler.transform(x_test)

x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=42,test_size=0.2)

ridge = Ridge(alpha=1.0)
ridge_model = ridge.fit(x_train,y_train)

lasso = Lasso(alpha=0.1)
lasso_model= lasso.fit(x_train,y_train)

#6. Build an SVR model to predict Sale price of the house.
SVR = SVR()
SVR_model = SVR.fit(x_train,y_train)

#7. Build Decision Tree Regressor to predict Sale price of the house.
DecisionTree = DecisionTreeRegressor()
DT_model = DecisionTree.fit(x_train,y_train)

#8. Build Random Forest Regression model to predict Sale price of the house.
RandomForest = RandomForestRegressor()
RF_model = RandomForest.fit(x_train,y_train)

#9.Use GridsearchCV and RandomizedsearchCV for tuning hyperparameters andfit your model on the optimal.

params = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [None, 10, 20, 30],
    'max_features': [ 'sqrt', 'log2'],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

#cv_object = StratifiedKFold(n_splits = 2)

RF_Randomized = RandomizedSearchCV(estimator=RF_model,param_distributions=params,n_iter=20,cv=2,scoring='neg_root_mean_squared_error',n_jobs=-1,random_state=42)
RF_Randomized.fit(x_train,y_train)

#Getting the best parameters from the grid search
print("Best Parameter Combination : {}".format(RF_Randomized.best_params_))

params = {
    'C': [1, 10, 100],
    'gamma': ['scale', 'auto'],
    'kernel': ['rbf']
}

#cv_object = StratifiedKFold(n_splits = 5,shuffle=True, random_state=42)
svr_grid = GridSearchCV(estimator = SVR_model, param_grid=params, cv=2, verbose = 0, return_train_score = True)
svr_grid.fit(x_train,y_train)

#Getting the best parameters from the grid search
print("Best Parameter Combination : {}".format(svr_grid.best_params_))

#10.Model Selection: Evaluate and compare performance of all the models tofindthebest model.

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

models = {
    'Ridge':ridge_model,
    'Lasso':lasso_model,
    'SVR':SVR_model,
    'Decision Tree': DT_model,
    'Random Forest': RF_model
}

results = []

for name, model in models.items():
    y_pre = model.predict(Xtest_scaled)
    mse = mean_squared_error(y_test, y_pre)
    mae = mean_absolute_error(y_test, y_pre)
    r2 = r2_score(y_test, y_pre)

    results.append({
        'Model': name,
        'MSE': mse,
        'MAE': mae,
        'R2 Score': r2
    })

import pandas as pd
results_df = pd.DataFrame(results)
print(results_df.sort_values(by='R2 Score', ascending=False))







#================ Task2==========================

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression

#For Charts
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt

#Preprocessing
from sklearn.preprocessing import OneHotEncoder, LabelEncoder,PolynomialFeatures
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import make_pipeline

from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report

data = pd.read_csv("/content/BankChurners.csv")
data.head()

data.info()

data.describe()

data.columns

data.isnull().sum()

data.duplicated().sum()

numeric_cols = data.select_dtypes(include=np.number).columns
numeric_cols

plt.figure(figsize=(12,8))
sns.heatmap(data[numeric_cols].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

sns.scatterplot(x="Total_Trans_Ct", y="Total_Trans_Amt",
                hue="Attrition_Flag", data=data, palette="Set1")
plt.title("Transaction Count vs Amount by Churn Status")
plt.show()

sns.pairplot(data,hue='Attrition_Flag',vars=['Total_Trans_Amt','Total_Trans_Ct','Avg_Open_To_Buy'])
plt.show()

plt.figure(figsize=(6,4))
sns.countplot(x="Attrition_Flag", data=data)
plt.title('Distribution Of Target Variable')
plt.show()

columns = ['Gender', 'Education_Level', 'Marital_Status',
       'Income_Category', 'Card_Category','Attrition_Flag']

locoder = LabelEncoder()
for i in columns:
   data[f"{i}_label"] = locoder.fit_transform(data[i])

data

data.info()

x = data.drop(['Gender','Education_Level','Marital_Status','Income_Category','Card_Category','Attrition_Flag','Attrition_Flag_label'],axis=1)
y = data['Attrition_Flag_label']

x_train, x_test,y_train, y_test = train_test_split(x,y,random_state=42,test_size=0.2)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

Logistic_Regression = LogisticRegression()
LR_model = Logistic_Regression.fit(x_train,y_train)

Naive_Bayes = GaussianNB()
NB_model = Naive_Bayes.fit(x_train,y_train)

KNN = KNeighborsClassifier()
Knn_model = KNN.fit(x_train,y_train)

SVM = SVC()
SVM_model = SVM.fit(x_train,y_train)

DecisionTree = DecisionTreeRegressor()
DT_model = DecisionTree.fit(x_train,y_train)

RandomForest = RandomForestRegressor()
RF_model = RandomForest.fit(x_train,y_train)

param_grid = {
    "C": [0.1, 1, 10],
    "gamma": ["scale", "auto"]
}

from sklearn.svm import SVC

grid = GridSearchCV(estimator=SVM_model,param_grid=param_grid,cv=5,scoring="accuracy",n_jobs=-1)

SVM_model=grid.fit(x_train,y_train)

print("Best Parameters:")
print(grid.best_params_)

best_svc = grid.best_estimator_

y_pre = best_svc.predict(x_test)

print("\n Accuracy:", accuracy_score(y_test, y_pre))
print("\n Classification Report:\n", classification_report(y_test, y_pre))

from sklearn.tree import DecisionTreeClassifier

DT_model = DecisionTreeClassifier()
params = {
    #'criterion': ['gini', 'entropy']
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid = GridSearchCV(estimator=DT_model,param_grid=params,cv=3,scoring="accuracy",n_jobs=1)

grid = GridSearchCV(DT_model, params, cv=5)
DT_model = grid.fit(x_train, y_train)

print("Best Parameters:")
print(grid.best_params_)

best_dt = grid.best_estimator_

y_pre = best_dt.predict(x_test)

print("\n Accuracy:", accuracy_score(y_test, y_pre))
print("\n Classification Report:\n", classification_report(y_test, y_pre))

from sklearn.ensemble import RandomForestClassifier
RF_model= RandomForestClassifier()

param_dist = {
    'n_estimators': np.arange(100, 1000, 100),      # number of trees
    'max_depth': [None, 5, 10, 20, 30],             # depth of each tree
    'min_samples_split': [2, 5, 10],                # split threshold
    'min_samples_leaf': [1, 2, 4],                  # leaf threshold
    'max_features': ['sqrt'],                           # feature sampling
    'bootstrap': [True, False]                      # sampling mode
}

random_search = RandomizedSearchCV(estimator=RF_model,param_distributions=param_dist,n_iter=20,cv=5,verbose=2,random_state=42,n_jobs=-1)

RF_model=random_search.fit(x_train, y_train)

print("Best Parameters Found:")
print(random_search.best_params_)

best_rf = random_search.best_estimator_

y_pre = best_rf.predict(x_test)

print("\n Accuracy:", accuracy_score(y_test, y_pre))
print("\n Classification Report:\n")
print(classification_report(y_test, y_pre))

def result(model):
  y_pre = model.predict(x_test)
  acc = accuracy_score(y_test, y_pre)
  prec = precision_score(y_test, y_pre)
  rec = recall_score(y_test, y_pre)
  f1 = f1_score(y_test, y_pre)
  return acc, prec, rec, f1

models = [LR_model,NB_model,Knn_model,SVM_model ,DT_model,RF_model]
for model in models:
  acc, prec, rec, f1 = result(model)
  print("\tmodel: ",model)
  print("Accuracy: ",acc)
  print("Precision: ",prec)
  print("Recall: ",rec)
  print("F1: ",f1)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

models = {
    'Logistic_Regression': LR_model,
    'Naive_Bayes': NB_model,
    'KNN': Knn_model,
    'SVM': SVM_model,
    'Decision_Tree': DT_model,
    'Random_Forest': RF_model,
}

results = []

for name, model in models.items():
    y_pre = model.predict(x_test)
    acc = accuracy_score(y_test, y_pre)
    prec = precision_score(y_test, y_pre)
    rec = recall_score(y_test, y_pre)
    f1 = f1_score(y_test, y_pre)


    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1': f1
    })

import pandas as pd
results_df = pd.DataFrame(results)
print(results_df.sort_values(by='F1', ascending=False))

